{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SUPPORT_TRASHOLD=0.005\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import re\n",
    "\n",
    "def findTimeStampAndRoundIt(s):\n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}', s)\n",
    "    date = datetime.datetime.strptime(match.group(), '%Y-%m-%d %H:%M:%S')\n",
    "    a, b = divmod(math.floor(date.minute/10)*10, 60)\n",
    "    date = date.replace(minute=b,second=0)\n",
    "    return date\n",
    "\n",
    "def removeTimestamp(row):\n",
    "    find = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n",
    "    start = re.search(find,row).start()\n",
    "    return row[0:start]\n",
    "\n",
    "def spliteToUserIdAndUserSearch(row,timestamp):\n",
    "    row = row.split(\"\\t\", 1)\n",
    "    row[1] =  row[1].rstrip('\\t')\n",
    "    return ((int(row[0]),timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")),row[1])\n",
    "\n",
    "def uniqueList(line):\n",
    "    uniqueSearches = set(line[1])\n",
    "    newLine = [line[0],list(uniqueSearches)]\n",
    "    return newLine\n",
    "\n",
    "log_txt=sc.textFile(\"user-searches-min.txt\")\n",
    "header = log_txt.first()\n",
    "\n",
    "#filter out the header, make sure the rest looks correct\n",
    "log_txt = log_txt.filter(lambda line: line != header)\n",
    "\n",
    "logSearch = log_txt.map(lambda line: spliteToUserIdAndUserSearch(removeTimestamp(line),findTimeStampAndRoundIt(line))).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No default accumulator param for type <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a348965ce20a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36maccumulator\u001b[0;34m(self, value, accum_param)\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0maccum_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccumulators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLEX_ACCUMULATOR_PARAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No default accumulator param for type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_accum_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_accum_id\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccum_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: No default accumulator param for type <class 'list'>"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "totalOfTransactions = logSearch.map(lambda q: (q[0], 1) ).reduceByKey(lambda c1,c2: c1+c2 ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(totalOfTransactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'b', 'a', 'a', 'c', 'c', 'myspace.com', 'dfdf', 'vaniqa.comh']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 0.3), ('b', 0.2), ('c', 0.2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the user id -> return only query \n",
    "all_queries = logSearch.map(lambda line: line[1])\n",
    "print(all_queries.take(10))\n",
    "# count how much time query is show for all the user divide by number of users\n",
    "suportX = all_queries.map(lambda q: (q, 1) ).reduceByKey(lambda c1,c2: c1+c2 ) \\\n",
    "                                                    .map(lambda x: (x[0], x[1] / totalOfTransactions)) \\\n",
    "                                                    .filter(lambda x: x[1] > SUPPORT_TRASHOLD)\n",
    "\n",
    "# suportX is list of queries that pass the thrasholds of support\n",
    "suportX.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique set of all queries\n",
    "validItems = suportX.map(lambda x:x[0]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_queries_by_support_thrasholds(inValidList,validItems):\n",
    "    return [s for s in inValidList if s in validItems]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ['a', 'b', 'c']), (2, ['b', 'a']), (3, ['a', 'c'])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = logSearch.map(lambda x: (x[0][0],x[1])).groupByKey().mapValues(list).filter(lambda kv: len(kv[1]) > 1)  \n",
    "user_query.take(20)\n",
    "user_query = user_query.map(lambda t: (t[0],include_queries_by_support_thrasholds(t[1],validItems)))\n",
    "user_query.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'), ('a', 'c'), ('b', 'c'), ('b', 'a'), ('a', 'c')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_pairs(arr):\n",
    "        result = []\n",
    "        for p1 in range(len(arr)):\n",
    "                for p2 in range(p1+1,len(arr)):\n",
    "                        result.append((arr[p1],arr[p2]))\n",
    "        return result\n",
    "    \n",
    "all_queries_pairs_tuples = user_query.map(lambda kv: kv[1]).flatMap(lambda arr: get_all_pairs(arr))\n",
    "all_queries_pairs_tuples.take(50)\n",
    "# userId | a, b ,c ,d\n",
    "# 1| 1,1,0,0->a,b\n",
    "# 2|0,1,1,0->b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 'b'), 0.2), (('a', 'c'), 0.2)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def sort_small_list(arr):\n",
    "    if(arr[0] <= arr[1]):\n",
    "        return arr\n",
    "    return [arr[1],arr[0]]\n",
    "# the sort is for (a,b) (b,a) = > (a,b) (a,b) => ((a,b),2)\n",
    "all_queries_tuples_sorted = all_queries_pairs_tuples.map(lambda kv: sort_small_list(list(kv))) \\\n",
    "                                                .map(lambda arr: (arr[0],arr[1]) )\n",
    "\n",
    "all_queries_pairs_tuples_count = all_queries_tuples_sorted.map(lambda kv: (kv,1)) \\\n",
    "                                                    .reduceByKey(lambda c1,c2: c1+c2 )\\\n",
    "                                                    .filter(lambda kv: kv[1] > 1) \\\n",
    "                                                    .map(lambda x: (x[0], x[1] / totalOfTransactions)) \n",
    "\n",
    "\n",
    "all_queries_pairs_tuples_count.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('a', 'b'), 0.2), ('a', 0.3)),\n",
       " ((('a', 'b'), 0.2), ('b', 0.2)),\n",
       " ((('a', 'b'), 0.2), ('c', 0.2))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_queries_tuples_cartesian = all_queries_pairs_tuples_count.cartesian(suportX)\n",
    "rdd_queries_tuples_cartesian.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'b', 0.6666666666666667), ('a', 'c', 0.6666666666666667)]\n",
      "[('b', 'a', 1.0), ('c', 'a', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "# this command calculate XUY/X by taking all lines that ((x ,y , number of suply(xUy)),(z ,number of suply(z))) when z ==x\n",
    "rdd_join_left = rdd_queries_tuples_cartesian.filter(lambda lr: lr[0][0][0] == lr[1][0]) \\\n",
    "                                            .map(lambda lr: (lr[0][0][0],lr[0][0][1],float(lr[0][1]) / lr[1][1]))\n",
    "\n",
    "print(rdd_join_left.take(5))\n",
    "# this command take XUY/Y\n",
    "\n",
    "rdd_join_right = rdd_queries_tuples_cartesian.filter(lambda lr: lr[0][0][1] == lr[1][0])\\\n",
    "                                             .map(lambda lr: (lr[0][0][1], lr[0][0][0], float(lr[0][1]) / lr[1][1]))\n",
    "print(rdd_join_right.take(5))\n",
    "\n",
    "rdd_query_conf = sc.union([rdd_join_left, rdd_join_right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 00:00:06\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
    "print(\"elapsed time: %s\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "####### TASK 2  #######\n",
    "#######################\n",
    "rddQueryConfDF = sqlContext.createDataFrame(rdd_query_conf, [\"X\", \"Y\",\"CONFIDENCE\"])\n",
    "rddQueryConfDF.coalesce(1).write.format('com.databricks.spark.csv').save('/home/kfir/Desktop/Ex2/Final/my.csv',header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "####### TASK 4.a ########\n",
    "#########################\n",
    "\n",
    "def filterConfidence(line,Conf):\n",
    "    if line[2] >=Conf:\n",
    "        return(True)\n",
    "    return(False)\n",
    "\n",
    "x_y_conf06 = rdd_query_conf.filter(lambda line: filterConfidence(line,0.6))\n",
    "x_y_conf08 = x_y_conf06.filter(lambda line: filterConfidence(line,0.8))\n",
    "x_y_conf09 = x_y_conf08.filter(lambda line:filterConfidence(line,0.9))\n",
    "print('the amount of related queries for 0.6 confidence')\n",
    "print(x_y_conf06.count())\n",
    "\n",
    "print('the amount of related queries for 0.8 confidence')\n",
    "print(x_y_conf08.count())\n",
    "\n",
    "print('the amount of related queries for 0.9 confidence')\n",
    "print(x_y_conf09.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getAllPairsWithMinConfidence(conf,obj):\n",
    "#     return obj.filter(lambda x: x[2]>=conf).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# print(getAllPairsWithMinConfidence(0.2,rdd_query_conf))\n",
    "# print(getAllPairsWithMinConfidence(0.4,rdd_query_conf))\n",
    "# print(getAllPairsWithMinConfidence(0.6,rdd_query_conf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
