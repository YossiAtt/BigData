{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORT_TRASHOLD=0.1\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "import time\n",
    "# df = spark.read.text(\"user-searches-min.txt\")\n",
    "import re\n",
    "\n",
    "def removeTimestamp(row):\n",
    "    find = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n",
    "    start = re.search(find,row).start()\n",
    "    return row[0:start]\n",
    "\n",
    "def spliteToUserIdAndUserSearch(row):\n",
    "    row = row.split(\"\\t\", 1)\n",
    "    row[1] =  row[1].rstrip('\\t')\n",
    "    return (int(row[0]),row[1])\n",
    "\n",
    "def uniqueList(line):\n",
    "    uniqueSearches = set(line[1])\n",
    "    newLine = [line[0],list(uniqueSearches)]\n",
    "    return newLine\n",
    "\n",
    "log_txt=sc.textFile(\"user-searches-min.txt\")\n",
    "header = log_txt.first()\n",
    "\n",
    "#filter out the header, make sure the rest looks correct\n",
    "log_txt = log_txt.filter(lambda line: line != header)\n",
    "logSearch = log_txt.map(lambda line: spliteToUserIdAndUserSearch(removeTimestamp(line))).distinct()\n",
    "# logSearch = logSearch.reduceByKey(lambda p,q: p+q).map(lambda line: uniqueList(line))\n",
    "# allWords = logSearch.flatMap(lambda x: x[1]).distinct()\n",
    "# allWords = allWords.map(lambda x: Row([x]))\n",
    "# allWordsDF = spark.createDataFrame(allWords, ['search'])\n",
    "# allWordsDF.show()\n",
    "print(logSearch.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "totalOfTransactions = logSearch.groupByKey().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(totalOfTransactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the user id -> return only query \n",
    "all_queries = logSearch.map(lambda line: line[1])\n",
    "# count how much time query is show for all the user divide by number of users\n",
    "rdd_query_count = all_queries.map(lambda q: (q, 1) ).reduceByKey(lambda c1,c2: c1+c2 ) \\\n",
    "                                                    .map(lambda x: (x[0], x[1] / totalOfTransactions)) \\\n",
    "                                                    .filter(lambda x: x[1] > SUPPORT_TRASHOLD)\n",
    "\n",
    "# rdd_query_count is list of queries that pass the thrasholds of support\n",
    "rdd_query_count.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the set of all queries\n",
    "validItems = rdd_query_count.map(lambda x:x[0]).collect()\n",
    "print(validItems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_queries_by_support_thrasholds(inValidList,validItems):\n",
    "    return [s for s in inValidList if s in validItems]\n",
    "user_query = logSearch.groupByKey().mapValues(list).filter(lambda kv: len(kv[1]) > 1)  \n",
    "user_query = user_query.map(lambda t: (t[0],include_queries_by_support_thrasholds(t[1],validItems)))\n",
    "user_query.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pairs(arr):\n",
    "        result = []\n",
    "        for p1 in range(len(arr)):\n",
    "                for p2 in range(p1+1,len(arr)):\n",
    "                        result.append((arr[p1],arr[p2]))\n",
    "        return result\n",
    "    \n",
    "all_queries_pairs_tuples = user_query.map(lambda kv: kv[1]).flatMap(lambda arr: get_all_pairs(arr))\n",
    "all_queries_pairs_tuples.take(50)\n",
    "# userId | a, b ,c ,d\n",
    "# 1| 1,1,0,0->a,b\n",
    "# 2|0,1,1,0->b,c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sort_small_list(arr):\n",
    "    if(arr[0] <= arr[1]):\n",
    "        return arr\n",
    "    return [arr[1],arr[0]]\n",
    "# the sort is for (a,b) (b,a) = > (a,b) (a,b) => ((a,b),2)\n",
    "all_queries_tuples_sorted = all_queries_pairs_tuples.map(lambda kv: sort_small_list(list(kv))) \\\n",
    "                                                .map(lambda arr: (arr[0],arr[1]) )\n",
    "\n",
    "all_queries_pairs_tuples_count = all_queries_tuples_sorted.map(lambda kv: (kv,1)) \\\n",
    "                                                    .reduceByKey(lambda c1,c2: c1+c2 )\\\n",
    "                                                    .filter(lambda kv: kv[1] > 1) \\\n",
    "                                                    .map(lambda x: (x[0], x[1] / totalOfTransactions)) \n",
    "\n",
    "\n",
    "all_queries_pairs_tuples_count.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_queries_tuples_cartesian = all_queries_pairs_tuples_count.cartesian(rdd_query_count)\n",
    "rdd_queries_tuples_cartesian.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this command calculate XUY/X by taking all lines that ((x ,y , number of suply(xUy)),(z ,number of suply(z))) when z ==x\n",
    "rdd_join_left = rdd_queries_tuples_cartesian.filter(lambda lr: lr[0][0][0] == lr[1][0]) \\\n",
    "                                            .map(lambda lr: (lr[0][0][0],lr[0][0][1],float(lr[0][1]) / lr[1][1]))\n",
    "\n",
    "print(rdd_join_left.take(5))\n",
    "# this command take XUY/Y\n",
    "\n",
    "rdd_join_right = rdd_queries_tuples_cartesian.filter(lambda lr: lr[0][0][1] == lr[1][0])\\\n",
    "                                             .map(lambda lr: (lr[0][0][1], lr[0][0][0], float(lr[0][1]) / lr[1][1]))\n",
    "print(rdd_join_right.take(5))\n",
    "\n",
    "rdd_query_conf = sc.union([rdd_join_left, rdd_join_right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "elapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
    "print(\"elapsed time: %s\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toCSVLine(data):\n",
    "      return ','.join(str(d) for d in data)\n",
    "\n",
    "lines = rdd_query_conf.map(toCSVLine)\n",
    "lines.saveAsTextFile('ex02_all_confs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = sc.textFile('ex02_all_confs')\n",
    "f_split = f1.map(lambda x: x.split(','))\n",
    "f_split_1 = f_split.map(lambda x: (x[0],x[1],float(x[2])))\n",
    "\n",
    "def getAllPairsWithMinConfidence(conf):\n",
    "    res = f_split_1.filter(lambda x: x[2]>=conf)\n",
    "    return res\n",
    "\n",
    "x_y_conf = getAllPairsWithMinConfidence(0.2)\n",
    "x_y_conf = getAllPairsWithMinConfidence(0.4)\n",
    "x_y_conf = getAllPairsWithMinConfidence(0.6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
